{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>INF580</h1></center>\n",
    "<h3 align = 'center'>Sentence similarity using graph-of-words representation embeddings</h3>\n",
    "<center><h5>Kniazev Leonid - Sattarov Tagir</h5></center>\n",
    "\n",
    "**Project idea 2**: Create a graph-of-words from a sentence, enrich it with semantic distances, then use the DG methods in these lectures to embed the graph in a low-dimensional space; then evaluate sentence similarity using vector angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import tqdm\n",
    "import numba\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.spatial.distance as dist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "from amplpy import AMPL\n",
    "import cvxpy as cp\n",
    "import dgp_sdp\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Datasets loading\n",
    "\n",
    "In order to create a graph-of-words from text corpus, we will use two datasets.\n",
    "\n",
    "The first is the pairs of english sentences and their similarities annotated by humans from SemEval-2017 contest (test set).\n",
    "\n",
    "USE_TRAIN is used to enrich our graph with some adiitional data. We chose 1M words dataset of movies subtitles as additionnal data from \"https://www.corpusdata.org/\" for our graph of words (train set). It has a lot of different styles of coversations and has ~20 times more lemmas than the test set from SemEval-2017. \n",
    "\n",
    "Below you can find the 5 first pairs of sentences. As we can see they are not long and we can suppose that in this case graphs will not give us a lot of information about sentences.\n",
    "\n",
    "At the same time, word-to-vec representation for a sentence will be meaningful and will keep information about most significant parts of sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TRAIN = True\n",
    "DAT_FILE = 'dist_graph.dat'\n",
    "\n",
    "with open('input.txt') as f:\n",
    "    lines_test = [line.strip() for line in f]\n",
    "with open('movies_text.txt') as f:\n",
    "    lines_train = [line.strip() for line in f]\n",
    "\n",
    "print(len(lines_test))\n",
    "print(lines_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocessing : remove punctuation, stopwords and lemmatize\n",
    "We will work with sentences without punctuation marks. We also remove stopwords and lemmatize all the words.\n",
    "\n",
    "Below we explain important variables that we used in our project:\n",
    "\n",
    "\"words\" is a dictionary with all the lemmas that we have in our test and train sets. word[\"cat\"] := index of \"cat\".\n",
    "\n",
    "\"test_words\" is the same thing with lemmas only from the test set.\n",
    "\n",
    "\"sentences_test\" is already cleaned test set with sentences represented as pairs of lists of word indexes.\n",
    "\n",
    "\"sentences_train\" the same thing for the train set.\n",
    "\n",
    "We use the information from the train dataset and we evaluate it on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "translation_table = str.maketrans('', '', string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = set(wordnet.all_lemma_names())\n",
    "\n",
    "words = dict()\n",
    "test_words = dict()\n",
    "sentences_test = list()\n",
    "\n",
    "for line in lines_test:\n",
    "    s1, s2 = line.split('\\t')\n",
    "\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "\n",
    "    for s, v in [(s1, v1), (s2, v2)]:\n",
    "        for w in s.split():\n",
    "            w = w.lower().translate(translation_table)\n",
    "            if w in stopwords_en:\n",
    "                continue\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            if w not in lemmas:\n",
    "                continue\n",
    "            words.setdefault(w, len(words))\n",
    "            test_words.setdefault(w, len(test_words))\n",
    "            v.append(words[w])\n",
    "\n",
    "    sentences_test.append((v1, v2))\n",
    "\n",
    "sentences_train = list()\n",
    "if USE_TRAIN:\n",
    "    for s in lines_train:\n",
    "        v = []\n",
    "    \n",
    "        for w in s.split():\n",
    "            w = w.lower().translate(translation_table)\n",
    "            if w in stopwords_en:\n",
    "                continue\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            if w not in lemmas:\n",
    "                continue\n",
    "            \n",
    "            words.setdefault(w, len(words))\n",
    "            v.append(words[w])\n",
    "\n",
    "        sentences_train.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words.keys()))\n",
    "print(len(test_words.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Main Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Make synonyms graph\n",
    "First, we create a graph representing synonyms. For that, we use **wordnet** synonyms.\n",
    "\n",
    "In **synonym_graph**, synonym_graph[i][j] is 1, iff the word words[i] and the word words[j] are synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_graph = np.zeros((len(words), len(words)))\n",
    "\n",
    "def set_of_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            syn_word = l.name()\n",
    "            if syn_word in stopwords_en:\n",
    "                continue\n",
    "            syn_word = lemmatizer.lemmatize(syn_word)\n",
    "            synonyms.append(syn_word)\n",
    "    return set(synonyms)\n",
    "\n",
    "for word, idx in words.items():\n",
    "    for syn in set_of_synonyms(word):\n",
    "        if syn not in words:\n",
    "            continue\n",
    "        syn_idx = words[syn]\n",
    "        synonym_graph[idx][syn_idx] = 1\n",
    "        synonym_graph[syn_idx][idx] = 1\n",
    "if USE_TRAIN:\n",
    "    synonym_graph = synonym_graph[:len(test_words), :len(test_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synonym_graph.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you an example of a set of synonyms for the first word in the dictionary.\n",
    "\n",
    "We see that there are 3 synonyms for \"person\" in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(words.keys())[0])\n",
    "print(set_of_synonyms(list(words.keys())[0]))\n",
    "print(sum(synonym_graph[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Make sliding window graph\n",
    "Now, we create a sliding window graph.\n",
    "\n",
    "**window_size** is a parameter. Each time two words words[i] and words[j] are in sliding window, we add 1 to window_graph[i][j]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "\n",
    "window_graph_train = np.zeros((len(words), len(words)))\n",
    "\n",
    "for ss in sentences_test:\n",
    "    for s in ss:\n",
    "        window = []\n",
    "        for i in range(len(s)):\n",
    "            window = window[-(window_size - 1):]\n",
    "            v1 = s[i]\n",
    "            for v2 in window:\n",
    "                window_graph_train[v1][v2] += 1\n",
    "                window_graph_train[v2][v1] += 1\n",
    "            window.append(v1)\n",
    "if USE_TRAIN:\n",
    "    for s in sentences_train:\n",
    "        window = []\n",
    "        for i in range(len(s)):\n",
    "            window = window[-(window_size - 1):]\n",
    "            v1 = s[i]\n",
    "            for v2 in window:\n",
    "                window_graph_train[v1][v2] += 1\n",
    "                window_graph_train[v2][v1] += 1\n",
    "            window.append(v1)\n",
    "if USE_TRAIN:\n",
    "    window_graph = window_graph_train[:len(test_words), :len(test_words)]\n",
    "else:\n",
    "    window_graph = window_graph_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(window_graph.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Make Spacy dependency tree graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a graph representing sentence syntactic.\n",
    "\n",
    "We work with **spaCy** dependency trees. **spaCy** uses the terms head and child to describe the words connected by a single arc in the dependency tree. Because the syntactic relations form a tree, every word has exactly one head. We can therefore iterate over the arcs in the tree by iterating over the words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_graph = np.zeros((len(test_words), len(test_words)))\n",
    "translation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "for i in range(len(lines_test)):\n",
    "    sent1, sent2 = lines_test[i].split('\\t')\n",
    "    sent1 = ' '.join([w for w in sent1.split()])\n",
    "    sent2 = ' '.join([w for w in sent2.split()])\n",
    "    sent1 = nlp(sent1)\n",
    "    sent2 = nlp(sent2)\n",
    "\n",
    "    for s in [sent1, sent2]:\n",
    "        for w in s: #iteration over the words in the sentence\n",
    "            w1 = w.text.lower().translate(translation_table)\n",
    "            w2 = w.head.text.lower().translate(translation_table)\n",
    "\n",
    "            if (not w1 in stopwords_en) & (not w2 in stopwords_en) & (w.dep_ != \"ROOT\") & (w.dep_ != \"punct\"):\n",
    "                w1 = lemmatizer.lemmatize(w1)\n",
    "                w2 = lemmatizer.lemmatize(w2)\n",
    "                if (w1 in test_words) & (w2 in test_words):\n",
    "                    dependency_graph[test_words[w1]][test_words[w2]] += 1\n",
    "                    dependency_graph[test_words[w2]][test_words[w2]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show you the number of words in syntactic dependacies with \"person\" in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(dependency_graph[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Combine synonym, window graphs and dependency graph\n",
    "\n",
    "Now, in order to use the information on synonyms, word order in sentences and syntactic dependencies, we put 3 graps together.\n",
    "\n",
    "Then we complete distance matrix with shortest path lengths using Floyd-Warshall algorithm.\n",
    "\n",
    "Knowing the idea of Floyd-Warshall algorithm, a simple check at the end proofs if graph is connected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synonym_graph.shape, window_graph.shape, dependency_graph.shape)\n",
    "distances = 1 / (synonym_graph + window_graph + dependency_graph + 1)\n",
    "#gr_max = np.max(synonym_graph + window_graph + dependency_graph)\n",
    "#distances = gr_max - (synonym_graph + window_graph + dependency_graph) + 1\n",
    "for i in range(distances.shape[0]):\n",
    "    for j in range(distances.shape[0]):\n",
    "        if distances[i][j] == 1:\n",
    "            distances[i][j] = -1.0\n",
    "\n",
    "np.fill_diagonal(distances, 0)\n",
    "\n",
    "def floyd_warshall(dist):\n",
    "    n = dist.shape[0]\n",
    "    for u in tqdm.trange(n):\n",
    "        for v in range(n):\n",
    "            if dist[u][v] == -1.0:\n",
    "                dist[u][v] = 10e6\n",
    "\n",
    "    @numba.jit\n",
    "    def h(dist, n, z):\n",
    "        for u in range(n):\n",
    "            for v in range(n):\n",
    "                if (dist[u][v] > dist[u][z] + dist[z][v]):\n",
    "                    dist[u][v] = dist[u][z] + dist[z][v]\n",
    "\n",
    "    for z in tqdm.trange(n):\n",
    "        h(dist, n, z)\n",
    "        \n",
    "    \n",
    "floyd_warshall(distances)\n",
    "\n",
    "if np.max(distances) == 10e6:\n",
    "    print(\"Distances graph is not connected\")\n",
    "else:\n",
    "    print(\"Distances graph is connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Distance Geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 MDS\n",
    "\n",
    "\n",
    "In this part we produce an approximate realisation of $x$.\n",
    "\n",
    "We have an approximate EDMs $\\tilde{D}$ from raw data completed with Floyd-Warshall algorithm. Then we compute an approximate Gram matrix:\n",
    "\n",
    "$$ \\tilde{G} = - \\frac{1}{2} J \\tilde{D}^2 J, $$\n",
    "\n",
    "where $J = I_n - \\frac{1}{n}\\mathbb{1}\\mathbb{1}^{T}$.\n",
    "\n",
    "Having spectral decomposition of $\\tilde{G} = P \\tilde{\\Lambda} P^{T}$, we find $\\Lambda$ the PSD diagonal matrix closest to $\\tilde{\\Lambda}$.\n",
    "\n",
    "$\\Lambda$ is obtained from $\\tilde{\\Lambda}$ by zeroing negative components.\n",
    "\n",
    "So, finally, $x = P \\sqrt{\\Lambda}$.\n",
    "\n",
    "To move $\\bar{x}$ to $0$, we reduce all the x by their mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def mds(matD, k):\n",
    "    N = matD.shape[0]\n",
    "    matJ = np.eye(N) - np.ones((N, N)) / N\n",
    "    matG = -0.5 * matJ @ (matD ** 2) @ matJ\n",
    "    vecL, matP = la.eigh(matG)\n",
    "    vecL[vecL < 0] = 0\n",
    "    return matP[:,-k:], vecL[-k:]\n",
    "\n",
    "def embed_graph(distances, k):\n",
    "    p, l = mds(distances, k)\n",
    "    result = p @ np.diag(np.sqrt(l))\n",
    "    result -= np.mean(result, axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function creates a .dat file in convinient for AMPL format based on subgraphs matrices\n",
    "def print_dat(distances):\n",
    "    with open(DAT_FILE, 'w') as f:\n",
    "        #sys.stdout = f # Change the standard output to the file we created.\n",
    "        f.write(\"param Kdim := 3;\\nparam n := {};\\nparam : E : c I :=\\n\".format(distances.shape[0]))\n",
    "        for i in tqdm.trange(distances.shape[0]):\n",
    "            for j in range(distances.shape[0]):\n",
    "                if i-j==0:\n",
    "                    continue\n",
    "                f.write(\"{} {} {} 1\\n\".format(i+1, j+1, distances[i, j]))\n",
    "        f.write(\";\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 General Pipeline\n",
    "\n",
    "We can follow these steps:\n",
    "\n",
    "1) We may use SDP relaxation for DGP. Its solution yields $n \\times n$ PSD matrix $X^{*}$.\n",
    "\n",
    "2) At the end, we need $n \\times K$ realization matrix $x^{*}$, so we apply PCA to $X^{*}$, keep $K$ largest comps, get $x^{\\prime}$.\n",
    "\n",
    "3) As this yields solutions with errors, we use $x^{\\prime}$ as starting point for local NLP solver.\n",
    "\n",
    "For the first part, we may also use DDP, an approximation for SDP. In this case, DDP could be infeasible.\n",
    "\n",
    "We may also use DualDDP which is a relaxation of SDP, and so, this is a relaxation of the original problem.\n",
    "\n",
    "Three possible methods would be:\n",
    "\n",
    "I. SDP -> PCA -> NLP\n",
    "\n",
    "II. DDP -> PCA -> NLP\n",
    "\n",
    "III. DualDDP -> PCA -> NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 SDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDP relaxation\n",
    "\n",
    "Let's consider the system of quadratic equations\n",
    "\n",
    "$$\n",
    "\\forall\\{u, v\\} \\in E \\quad\\left\\|x_{u}-x_{v}\\right\\|^{2}=d_{u v}^{2}\n",
    "$$\n",
    "\n",
    "We can add a new variable matrix $X$ s.t. $X_{i j} = x_i x_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\Rightarrow \\forall\\{i, j\\} \\in E \\quad\\left\\|x_{i}\\right\\|_{2}^{2}+\\left\\|x_{j}\\right\\|_{2}^{2}-2 x_{i} \\cdot x_{j}=d_{i j}^{2} \\\\\n",
    "&\\Rightarrow\\left\\{\\begin{aligned}\n",
    "\\forall\\{i, j\\} \\in E \\quad X_{i i}+X_{j j}-2 X_{i j} &=d_{i j}^{2} \\\\\n",
    "X &=x x^{\\top}\n",
    "\\end{aligned}\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we will add a relaxation, so we change $X-x x^{\\top}=0$ to $X-x x^{\\top} \\succeq 0$.\n",
    "\n",
    "Then we can replace $\\operatorname{Schur}(X, x)=\\left(\\begin{array}{cc}\n",
    "I_{K} & x^{\\top} \\\\\n",
    "x & X\n",
    "\\end{array}\\right)\\succeq 0$ by $$X \\succeq 0,$$ because $x$ is not used elsewhere.\n",
    "\n",
    "One of proposed methods gives an MP\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min F \\bullet X =\n",
    "\\operatorname{tr}\\left(F^{\\top} X\\right)\n",
    "& \\\\\n",
    "\\forall\\{i, j\\} \\in E \\quad X_{i i}+X_{j j}-2 X_{i j} &=d_{i j}^{2} \\\\\n",
    "X & \\succeq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We should choose the objective function, so we use a common one:\n",
    "\n",
    "$$\n",
    "\\min \\sum_{\\{i, j\\} \\in E}\\left(X_{i i}+X_{j j}-2 X_{i j}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdp(DAT_FILE, showplot=False):\n",
    "    (Kdim, n, E) = dgp_sdp.readDat('./' + DAT_FILE)\n",
    "\n",
    "    ## construct weighted vertex neighbourhoods from edges\n",
    "    G = {i:dict() for i in range(n)}\n",
    "    for e in E:\n",
    "        i = e[0]-1 # first vertex\n",
    "        j = e[1]-1 # second vertex\n",
    "        w = e[2] # edge weight\n",
    "        if i > j:\n",
    "            t = i\n",
    "            i = j\n",
    "            j = t\n",
    "        G[i][j] = w\n",
    "\n",
    "    ## MP formulation: vars, obj, constrs\n",
    "    X = cp.Variable((n,n), PSD=True)\n",
    "\n",
    "    cobj1 = sum([X[i,i] + X[j,j] - 2*X[i,j] for i in range(n) for j in G[i] if i<j])\n",
    "    objective = cp.Minimize(cobj1)\n",
    "\n",
    "    constraints = [X[i,i] + X[j,j] - 2*X[i,j] == G[i][j]**2 for i in range(n) for j in G[i] if i<j]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "\n",
    "    ## solve the problem\n",
    "    prob.solve(solver=cp.SCS, verbose=True)\n",
    "\n",
    "    objfunval = prob.value\n",
    "    print(\"optimal obj. fun. value =\", objfunval)\n",
    "\n",
    "    ## retrieve realization in K dimensions\n",
    "    print(\"ambient dimension n =\", n)\n",
    "    if X.value is None:\n",
    "        A = np.zeros((n,n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                A[i,j] = np.random.random()\n",
    "        Y = dgp_sdp.MDS(A)\n",
    "    else:\n",
    "        Y = dgp_sdp.MDS(X.value)\n",
    "    K = Y.shape[1]\n",
    "    print(\"found relaxed embedding in natural dimension K =\", K)\n",
    "    if K not in {2,3}:\n",
    "        if K < 2:\n",
    "            K = 2\n",
    "        elif K > 3:\n",
    "            K = 3\n",
    "    print(\"now projecting to\", K, \"principal dimensions\")\n",
    "    if X.value is None:\n",
    "        xbar = dgp_sdp.PCA(A, K)\n",
    "    else:\n",
    "        xbar = dgp_sdp.PCA(X.value, K)\n",
    "        \n",
    "    ## report SDP solution statistics\n",
    "    mderr1 = dgp_sdp.mde(xbar, G)\n",
    "    print(\"SDP mean distance error =\", mderr1)\n",
    "    lderr1 = dgp_sdp.lde(xbar, G)\n",
    "    print(\"SDP largest distance error =\", lderr1)\n",
    "\n",
    "    from amplpy import Environment\n",
    "\n",
    "    ## refine solution with a local NLP solver\n",
    "    nlp = AMPL(Environment('../ampl'))\n",
    "    nlp.read(\"dgp.mod\")\n",
    "    nlp.readData('./' + DAT_FILE)\n",
    "    nlp.setOption('solver', dgp_sdp.NLPsolver)\n",
    "    xvar = nlp.getVariable('x')\n",
    "    for i in range(n):\n",
    "        for k in range(K):\n",
    "            xvar[i+1,k+1].setValue(xbar[i,k])\n",
    "    nlp.solve()\n",
    "    xvar = nlp.getVariable('x')\n",
    "    x = np.zeros((n,K))\n",
    "    for i in range(n):\n",
    "        for k in range(K):\n",
    "            x[i,k] = xvar[i+1,k+1].value()\n",
    "\n",
    "    # report NLP solution statistics\n",
    "    mderr2 = dgp_sdp.mde(x, G)\n",
    "    print(\"NLP mean distance error =\", mderr2)\n",
    "    lderr2 = dgp_sdp.lde(x, G)\n",
    "    print(\"NLP largest distance error =\", lderr2)\n",
    "\n",
    "    ## plot results\n",
    "    if dgp_sdp.showplot:\n",
    "        if K == 2:\n",
    "            plt.scatter(x[:,0], x[:,1])\n",
    "            plt.plot(x[:,0], x[:,1])\n",
    "        elif K == 3:\n",
    "            fig = plt.figure()\n",
    "            ax = Axes3D(fig)\n",
    "            ax.scatter(x[:,0], x[:,1], x[:,2])\n",
    "            ax.plot(x[:,0], x[:,1], x[:,2])\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_zlabel('z')    \n",
    "        plt.show()\n",
    "    return (x, X.value is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 DDP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDP solver is a bottleneck in the Method 3.2.1. To avoid testing a matrix for SDP, we would like to work with LP problem instead.\n",
    "\n",
    "For this reason, we introduce diagonally dominant (DD) matrices:\n",
    "\n",
    "$n \\times n$ symmetric matrix $X$ is DD if\n",
    "$$\n",
    "\\forall i \\leq n \\quad X_{i i} \\geq \\sum_{j \\neq i}\\left|X_{i j}\\right| \\quad (*)\n",
    "$$\n",
    "\n",
    "Every DD matrix is PSD.\n",
    "\n",
    "Before we had an MP such that\n",
    "\n",
    "\\begin{aligned}\n",
    "\\forall\\{i, j\\} \\in E \\quad X_{i i}+X_{j j}-2 X_{i j} &=d_{i j}^{2} \\\\\n",
    "X & \\succeq 0\n",
    "\\end{aligned}\n",
    "\n",
    "Our approximation gives\n",
    "\n",
    "\\begin{aligned}\n",
    "\\forall\\{i, j\\} \\in E \\quad X_{i i}+X_{j j}-2 X_{i j} &=d_{i j}^{2} \\\\\n",
    "X & \\text{ is DD} \\quad (*)\n",
    "\\end{aligned}\n",
    "\n",
    "We may linearize (*) consrtaints by using an additional matrix variable $T$:\n",
    "\n",
    "$$\n",
    "(*) \\Rightarrow \\begin{aligned}\n",
    "\\forall i \\leq n & \\sum_{\\substack{j \\leq n \\\\\n",
    "j \\neq i}} T_{i j} \\leq X_{i i} \\\\\n",
    "& -T \\leq X\\leq T \\\\\n",
    "& T \\geq 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Considering the fact that DDP approximation could be infeasible, we will enlarge the feasible region.\n",
    "\n",
    "We make a change:\n",
    "\n",
    "$$\n",
    "\\forall\\{i, j\\} \\in E \\quad X_{i i}+X_{j j}-2 X_{i j}=d_{i j}^{2}\n",
    "$$\n",
    "\n",
    "is now relaxed\n",
    "\n",
    "$$\n",
    "\\forall\\{i, j\\} \\in E \\quad X_{i i}+X_{j j}-2 X_{i j} \\geq d_{i j}^{2}\n",
    "$$\n",
    "\n",
    "An SDP problem becomes linear, but we need to use a specific objective function due to introduced relaxation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min \\sum_{\\{i, j\\} \\in E}\\left(X_{i i}+X_{j j}-2 X_{i j}\\right)\\\\\n",
    "&\\begin{aligned}\n",
    "\\forall\\{i, j\\} \\in E \\quad X_{i i}+X_{j j}-2 X_{i j} & \\geq d_{i j}^{2} \\\\\n",
    "\\forall i \\leq n \\quad \\sum_{j \\leq n, i \\neq j} T_{i j} & \\leq X_{i i} \\\\\n",
    "-T \\leq X & \\leq T \\\\\n",
    "T & \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddp(DAT_FILE, showplot=False):\n",
    "    # read instance\n",
    "    (Kdim, n, E) = dgp_sdp.readDat('./' + DAT_FILE)\n",
    "\n",
    "    ## construct weighted vertex neighbourhoods from edges\n",
    "    G = {i:dict() for i in range(n)}\n",
    "    for e in E:\n",
    "        i = e[0]-1 # first vertex\n",
    "        j = e[1]-1 # second vertex\n",
    "        w = e[2] # edge weight\n",
    "        if i > j:\n",
    "            t = i\n",
    "            i = j\n",
    "            j = t\n",
    "        G[i][j] = w \n",
    "\n",
    "    ## formulate and solve the dual DDP\n",
    "    ddp = AMPL(Environment('../ampl'))\n",
    "    ddp.read(\"dgp_ddp.mod\")\n",
    "    ddp.readData('./' + DAT_FILE)\n",
    "    ddp.setOption('solver', dgp_sdp.LPsolver)\n",
    "    ddp.solve()\n",
    "    objfun = ddp.getObjective('push')\n",
    "    objfunval = objfun.value()\n",
    "    print(\"optimal obj. fun. value =\", objfunval)\n",
    "    Xvar = ddp.getVariable('X')\n",
    "    X = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            X[i,j] = Xvar[i+1,j+1].value()\n",
    "\n",
    "    ## retrieve realization in K dimensions\n",
    "    print(\"ambient dimension n =\", n)\n",
    "    Y = dgp_sdp.MDS(X)\n",
    "    K = Y.shape[1]\n",
    "    print(\"found relaxed embedding in natural dimension K =\", K)\n",
    "    if K not in {2,3}:\n",
    "        if K < 2:\n",
    "            K = 2\n",
    "        elif K > 3:\n",
    "            K = 3\n",
    "    print(\"now projecting to\", K, \"principal dimensions\")\n",
    "    xbar = dgp_sdp.PCA(X, K)\n",
    "\n",
    "    ## report dualDDP solution statistics\n",
    "    mderr1 = dgp_sdp.mde(xbar, G)\n",
    "    print(\"DDP mean distance error =\", mderr1)\n",
    "    lderr1 = dgp_sdp.lde(xbar, G)\n",
    "    print(\"DDP largest distance error =\", lderr1)\n",
    "\n",
    "    ## refine solution with a local NLP solver\n",
    "    nlp = AMPL(Environment('../ampl'))\n",
    "    nlp.read(\"dgp.mod\")\n",
    "    nlp.readData('./' + DAT_FILE)\n",
    "    nlp.setOption('solver', dgp_sdp.NLPsolver)\n",
    "    xvar = nlp.getVariable('x')\n",
    "    for i in range(n):\n",
    "        for k in range(K):\n",
    "            xvar[i+1,k+1].setValue(xbar[i,k])\n",
    "    nlp.solve()\n",
    "    xvar = nlp.getVariable('x')\n",
    "    x = np.zeros((n,K))\n",
    "    for i in range(n):\n",
    "        for k in range(K):\n",
    "            x[i,k] = xvar[i+1,k+1].value()\n",
    "\n",
    "    # report NLP solution statistics\n",
    "    mderr2 = dgp_sdp.mde(x, G)\n",
    "    print(\"NLP mean distance error =\", mderr2)\n",
    "    lderr2 = dgp_sdp.lde(x, G)\n",
    "    print(\"NLP largest distance error =\", lderr2)\n",
    "\n",
    "    ## plot results\n",
    "    if showplot:\n",
    "        if K == 2:\n",
    "            plt.scatter(x[:,0], x[:,1])\n",
    "            plt.plot(x[:,0], x[:,1])\n",
    "        elif K == 3:\n",
    "            fig = plt.figure()\n",
    "            ax = Axes3D(fig)\n",
    "            ax.scatter(x[:,0], x[:,1], x[:,2])\n",
    "            ax.plot(x[:,0], x[:,1], x[:,2])\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_zlabel('z')    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Dual Cone DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let C be a cone if:\n",
    "$$\\forall A, B \\in C, \\alpha, \\beta \\geq 0 \\quad \\alpha A+\\beta B \\in C$$\n",
    "and $C^{*}=\\{y \\mid \\forall x \\in C \\quad \\langle x, y\\rangle \\geq 0\\}$ its dual cone.\n",
    "\n",
    "We will use $\\mathcal{X}_{\\mathbb{D D}}=\\left\\{e_{i} \\mid i \\leq n\\right\\} \\cup\\left\\{\\left(e_{i} \\pm e_{j}\\right) \\mid i<j \\leq n\\right\\}$ where $e_i$ is the $i^{th}$ basis element of $\\R^n$.\n",
    "\n",
    "$\\mathcal{X}_{\\mathbb{D D}}$ finitely generates the set of Diagonally Dominant matrices which is denoted as $\\mathbb{D D}$. We mention that $|\\mathcal{X}_{\\mathbb{D D}}| = |O(n^2)|$\n",
    "\n",
    "Using the finitely generated dual cone theorem, we know that $$DD^* = \\left\\{Y\\mid \\forall x \\in \\mathcal{X}_{\\mathbb{D D}}\\left(Y \\bullet x x^{\\top} \\geq 0\\right) = \\left(x^{\\top} Y x \\geq 0\\right) \\right\\}$$\n",
    "\n",
    "As $\\mathbb{D D}^{*} \\supset \\mathbb{P S D}$, we may use an relaxation of SDP.\n",
    "\n",
    "So, we change the formulation of SDP for our DGP and we take \n",
    "condition $\\forall v \\in \\mathcal{X}_{\\mathbb{D D}}~~ v^{\\top} X v \\geq 0$ instead of $ X \\succeq 0$. \n",
    "\n",
    "And the final Dual cone DDP formulation for DGP is\n",
    "$$\\min \\sum_{\\{i, j\\} \\in E}\\left(X_{i i}+X_{j j}-2 X_{i j}\\right)\\\\\n",
    "\\forall\\{i, j\\} \\in E ~~~~  X_{i i}+X_{j j}-2 X_{i j} =d_{i j}^{2}  \\\\\n",
    "\\forall i \\leq n \\quad X_{i i} \\geq 0\\\\\n",
    "\\forall\\{i, j\\} \\notin E \\quad X_{i i}+X_{j j}-2 X_{i j} \\geq 0\\\\\n",
    "\\forall i<j \\quad X_{i i}+X_{j j}+2 X_{i j} \\geq 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dualddp(DAT_FILE, showplot=False):\n",
    "    # read instance\n",
    "    (Kdim, n, E) = dgp_sdp.readDat('./' + DAT_FILE)\n",
    "\n",
    "    ## construct weighted vertex neighbourhoods from edges\n",
    "    G = {i:dict() for i in range(n)}\n",
    "    for e in E:\n",
    "        i = e[0]-1 # first vertex\n",
    "        j = e[1]-1 # second vertex\n",
    "        w = e[2] # edge weight\n",
    "        if i > j:\n",
    "            t = i\n",
    "            i = j\n",
    "            j = t\n",
    "        G[i][j] = w \n",
    "\n",
    "    ## formulate and solve the dual DDP\n",
    "    dualddp = AMPL()\n",
    "    dualddp.read(\"dgp_dualddp.mod\")\n",
    "    dualddp.readData('./' + DAT_FILE)\n",
    "    dualddp.setOption('solver', dgp_sdp.LPsolver)\n",
    "    dualddp.solve()\n",
    "    objfun = dualddp.getObjective('push')\n",
    "    objfunval = objfun.value()\n",
    "    print(\"optimal obj. fun. value =\", objfunval)\n",
    "    Xvar = dualddp.getVariable('X')\n",
    "    X = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            X[i,j] = Xvar[i+1,j+1].value()\n",
    "\n",
    "    ## retrieve realization in K dimensions\n",
    "    print(\"ambient dimension n =\", n)\n",
    "    Y = dgp_sdp.MDS(X)\n",
    "    K = Y.shape[1]\n",
    "    print(\"found relaxed embedding in natural dimension K =\", K)\n",
    "    if K not in {2,3}:\n",
    "        if K < 2:\n",
    "            K = 2\n",
    "        elif K > 3:\n",
    "            K = 3\n",
    "    print(\"now projecting to\", K, \"principal dimensions\")\n",
    "    xbar = dgp_sdp.PCA(X, K)\n",
    "\n",
    "    ## report dualDDP solution statistics\n",
    "    mderr1 = dgp_sdp.mde(xbar, G)\n",
    "    print(\"dualDDP mean distance error =\", mderr1)\n",
    "    lderr1 = dgp_sdp.lde(xbar, G)\n",
    "    print(\"dualDDP largest distance error =\", lderr1)\n",
    "\n",
    "    ## refine solution with a local NLP solver\n",
    "    nlp = AMPL()\n",
    "    nlp.read(\"dgp.mod\")\n",
    "    nlp.readData('./' + DAT_FILE)\n",
    "    nlp.setOption('solver', dgp_sdp.NLPsolver)\n",
    "    xvar = nlp.getVariable('x')\n",
    "    for i in range(n):\n",
    "        for k in range(K):\n",
    "            xvar[i+1,k+1].setValue(xbar[i,k])\n",
    "    nlp.solve()\n",
    "    xvar = nlp.getVariable('x')\n",
    "    x = np.zeros((n,K))\n",
    "    for i in range(n):\n",
    "        for k in range(K):\n",
    "            x[i,k] = xvar[i+1,k+1].value()\n",
    "\n",
    "    # report NLP solution statistics\n",
    "    mderr2 = dgp_sdp.mde(x, G)\n",
    "    print(\"NLP mean distance error =\", mderr2)\n",
    "    lderr2 = dgp_sdp.lde(x, G)\n",
    "    print(\"NLP largest distance error =\", lderr2)\n",
    "\n",
    "    ## plot results\n",
    "    if showplot:\n",
    "        if K == 2:\n",
    "            plt.scatter(x[:,0], x[:,1])\n",
    "            plt.plot(x[:,0], x[:,1])\n",
    "        elif K == 3:\n",
    "            fig = plt.figure()\n",
    "            ax = Axes3D(fig)\n",
    "            ax.scatter(x[:,0], x[:,1], x[:,2])\n",
    "            ax.plot(x[:,0], x[:,1], x[:,2])\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_zlabel('z')    \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Visualisation in 2D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next plot gives us a visualisation of the result on 2D. Then we will use k = 20 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "word_to_vec = embed_graph(distances, k)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(*embed_graph(distances, 2).transpose())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application of methods for  for subgraphs l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_printed_sents = 250\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "with open('train.txt') as f:\n",
    "    train = np.array([float(line.strip()) for line in f])\n",
    "ampl_res = []\n",
    "idx = []\n",
    "\n",
    "for i, (s1, s2) in enumerate(sentences_test):\n",
    "    if (len(s1) < 3) or (len(s2) < 3):\n",
    "        ampl_res.append(0)\n",
    "        continue\n",
    "    #if i > N_printed_sents:\n",
    "    #    break\n",
    "    subgraph1 = distances[np.ix_(s1, s1)]\n",
    "    subgraph2 = distances[np.ix_(s2, s2)]\n",
    "    print_dat(subgraph1)\n",
    "    #result1, suc1 = sdp(DAT_FILE, showplot=False)\n",
    "    #result1, suc1 = ddp(DAT_FILE, showplot=False)\n",
    "    result1, suc1 = dualddp(DAT_FILE, showplot=False)\n",
    "    print_dat(subgraph2)\n",
    "    #result2, suc2 = sdp(DAT_FILE, showplot=False)\n",
    "    #result2, suc2 = ddp(DAT_FILE, showplot=False)\n",
    "    result2, suc2 = dualddp(DAT_FILE, showplot=False)\n",
    "    if not (suc1 & suc2):\n",
    "        ampl_res.append(0)\n",
    "        continue\n",
    "    v1 = np.mean(result1, axis=0)\n",
    "    v2 = np.mean(result2, axis=0)\n",
    "    if v1.shape != v2.shape:\n",
    "        ampl_res.append(0)\n",
    "        continue\n",
    "    d = (cosine_similarity([v1], [v2])[0,0]+1)/2.\n",
    "    ampl_res.append(d)\n",
    "    print(i, \"{:1.8f} {:1.8f}\".format(train[i] / 5, d), lines_test[i], sep='\\t')\n",
    "    idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_xy = np.corrcoef(np.array(train)[idx], np.array(ampl_res)[idx])\n",
    "print(\"p_xy spacy train = \", p_xy[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(idx)/250)\n",
    "#percentage of concidered sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_printed_sents = 2\n",
    "#nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "with open('train.txt') as f:\n",
    "    train = np.array([float(line.strip()) for line in f])\n",
    "\n",
    "for i, (s1, s2) in enumerate(sentences_test):\n",
    "    v1 = np.mean(word_to_vec[s1,:], axis=0)\n",
    "    v2 = np.mean(word_to_vec[s2,:], axis=0)\n",
    "    d = (cosine_similarity([v1], [v2])[0,0]+1)/2.\n",
    "\n",
    "\n",
    "\n",
    "    #print(d.shape)\n",
    "    sent1, sent2 = lines_test[i].split('\\t')\n",
    "    sent1 = nlp(sent1)\n",
    "    sent1 = nlp(' '.join([str(t) for t in sent1 if not t.is_stop]))\n",
    "    sent2 = nlp(sent2)\n",
    "    sent2 = nlp(' '.join([str(t) for t in sent2 if not t.is_stop]))\n",
    "    #sent1 = nlp(' '.join([word for word in sent1 if word not in english_stopwords]))\n",
    "    #sent2 = nlp(' '.join([word for word in sent2 if word not in english_stopwords]))\n",
    "    similarity = sent1.similarity(sent2)\n",
    "    #if similarity < 0.4:\n",
    "    print(i, \"{:1.8f} {:1.8f} {:1.8f}\".format(train[i] / 5, d, similarity), lines_test[i], sep='\\t')\n",
    "    if i > N_printed_sents:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_simlarity_n = 1-np.array([dist.cosine(np.mean(word_to_vec[s1,:], axis=0), np.mean(word_to_vec[s2,:], axis=0)) for (s1, s2) in sentences_test])/2\n",
    "spacy_similarity_n = np.array([nlp(' '.join([str(t) for t in nlp(sent2) if not t.is_stop])).similarity(nlp(' '.join([str(t) for t in nlp(sent1) if not t.is_stop]))) for (sent1, sent2) in [s.split('\\t') for s in lines_test]])\n",
    "print(spacy_similarity_n[:10])\n",
    "\n",
    "print(our_simlarity_n[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pearson correlation coefficient\n",
    "To estimate the quality of models we will use this coefficient, which is calculated as follows:\n",
    "$$p_{XY} = \\frac{\\sigma_{XY}}{\\sigma_X \\cdot \\sigma_Y}$$\n",
    "where $p_{XY}$ is the Pearson correlation coefficient, $\\sigma_Y, \\sigma_X$ are standart deviations of $Y$ and $X$, and $\\sigma_{XY}$ is the covariance between $X$ and $Y$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we normalize all the \n",
    "p_xy = np.corrcoef(train, spacy_similarity_n)\n",
    "print(\"p_xy spacy train = \", p_xy[1, 0])\n",
    "p_xy = np.corrcoef(our_simlarity_n, spacy_similarity_n)\n",
    "print(\"p_xy spacy our_simlarity_n = \", p_xy[1, 0])\n",
    "p_xy = np.corrcoef(train, our_simlarity_n)\n",
    "print(\"p_xy our_simlarity_n train = \", p_xy[1, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Our comments on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the best quality we have for spacy built-in function which uses word-to-vec representation. The second best and not too far from spacy, is our implementation of MDS on the graph representation of all the sentences.\n",
    "As it was expected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Further researches and possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an improvement one could use all the DG methods on the graph with <b>all the words</b> as in the MDS. \n",
    "\n",
    "Another problem that we had is that words from the dataset could close after lemmatizing when before they were not, and this problem is more pronounced when we have \"strange\" vocabulary as we have in our additional dataset. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f07a3c1acea75c2d4c353e9cd3dd42ec72fb863baf9770b89be62cf6c327390c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
