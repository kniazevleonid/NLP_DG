{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\satt-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\satt-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\satt-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import tqdm\n",
    "import numba\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial.distance as dist\n",
    "import spacy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Datasets loading\n",
    "USE_TRAIN is used to \"train\" our graph on some adiitional data.\n",
    "We print first 10 pairs to give you an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "USE_TRAIN = True\n",
    "DAT_FILE = 'dist_graph.dat'\n",
    "with open('input.txt') as f:\n",
    "    lines_test = [line.strip() for line in f]\n",
    "with open('movies_text.txt') as f:\n",
    "    lines_train = [line.strip() for line in f]\n",
    "\n",
    "print(len(lines_test))\n",
    "#lines_train = lines_train[:30]\n",
    "#print(lines_movies[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Preprocessing : remove punctuation, stopwords and lemmatize\n",
    "We will work with sentences without punctuation marks. We also remove stopwords and lemmatize all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "translation_table = str.maketrans('', '', string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = set(wordnet.all_lemma_names())\n",
    "\n",
    "words = dict()\n",
    "test_words = dict()\n",
    "sentences_test = list()\n",
    "\n",
    "for line in lines_test:\n",
    "    s1, s2 = line.split('\\t')\n",
    "\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "\n",
    "    for s, v in [(s1, v1), (s2, v2)]:\n",
    "        for w in s.split():\n",
    "            w = w.lower().translate(translation_table)\n",
    "            if w in stopwords_en:\n",
    "                continue\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            if w not in lemmas:\n",
    "                continue\n",
    "            words.setdefault(w, len(words))\n",
    "            test_words.setdefault(w, len(test_words))\n",
    "            v.append(words[w])\n",
    "\n",
    "    sentences_test.append((v1, v2))\n",
    "\n",
    "sentences_train = list()\n",
    "if USE_TRAIN:\n",
    "    for s in lines_train:\n",
    "        #s1, s2 = line.split('\\t')\n",
    "        \n",
    "        v = []\n",
    "        \n",
    "\n",
    "        for w in s.split():\n",
    "            w = w.lower().translate(translation_table)\n",
    "            if w in stopwords_en:\n",
    "                continue\n",
    "            w = lemmatizer.lemmatize(w)\n",
    "            if w not in lemmas:\n",
    "                continue\n",
    "            \n",
    "            words.setdefault(w, len(words))\n",
    "            v.append(words[w])\n",
    "\n",
    "        sentences_train.append(v)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18669\n",
      "687\n"
     ]
    }
   ],
   "source": [
    "print(len(words.keys()))\n",
    "print(len(test_words.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Main Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Make synonyms graph\n",
    "First, we create a graph representing synonyms. For that, we use **wordnet** synonyms.\n",
    "\n",
    "In **synonym_graph**, synonym_graph[i][j] is 1, iff the word words[i] and the word words[j] are synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_graph = np.zeros((len(words), len(words)))\n",
    "\n",
    "def set_of_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            syn_word = l.name()\n",
    "            if syn_word in stopwords_en:\n",
    "                continue\n",
    "            syn_word = lemmatizer.lemmatize(syn_word)\n",
    "            synonyms.append(syn_word)\n",
    "    return set(synonyms)\n",
    "\n",
    "for word, idx in words.items():\n",
    "    for syn in set_of_synonyms(word):\n",
    "        if syn not in words:\n",
    "            continue\n",
    "        syn_idx = words[syn]\n",
    "        synonym_graph[idx][syn_idx] = 1\n",
    "        synonym_graph[syn_idx][idx] = 1\n",
    "if USE_TRAIN:\n",
    "    synonym_graph = synonym_graph[:len(test_words), :len(test_words)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you an example of a set of synonyms for the first word in the dictionary.\n",
    "\n",
    "We see that there are 3 synonyms for \"person\" in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "{'soul', 'individual', 'mortal', 'person', 'someone', 'somebody'}\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(list(words.keys())[0])\n",
    "print(set_of_synonyms(list(words.keys())[0]))\n",
    "print(sum(synonym_graph[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Make sliding window graph\n",
    "Now, we create a sliding window graph.\n",
    "\n",
    "**window_size** is a parameter. Each time two words words[i] and words[j] are in sliding window, we add 1 to window_graph[i][j]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "\n",
    "window_graph_train = np.zeros((len(words), len(words)))\n",
    "\n",
    "for ss in sentences_test:\n",
    "    for s in ss:\n",
    "        window = []\n",
    "        for i in range(len(s)):\n",
    "            window = window[-(window_size - 1):]\n",
    "            v1 = s[i]\n",
    "            for v2 in window:\n",
    "                window_graph_train[v1][v2] += 1\n",
    "                window_graph_train[v2][v1] += 1\n",
    "            window.append(v1)\n",
    "if USE_TRAIN:\n",
    "    for s in sentences_train:\n",
    "        window = []\n",
    "        for i in range(len(s)):\n",
    "            window = window[-(window_size - 1):]\n",
    "            v1 = s[i]\n",
    "            for v2 in window:\n",
    "                window_graph_train[v1][v2] += 1\n",
    "                window_graph_train[v2][v1] += 1\n",
    "            window.append(v1)\n",
    "if USE_TRAIN:\n",
    "    window_graph = window_graph_train[:len(test_words), :len(test_words)]\n",
    "else:\n",
    "    window_graph = window_graph_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(687, 687)\n"
     ]
    }
   ],
   "source": [
    "print(window_graph.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Make Spacy dependency tree graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a graph representing sentence syntactic.\n",
    "\n",
    "We work with **spaCy** dependency trees. **spaCy** uses the terms head and child to describe the words connected by a single arc in the dependency tree. Because the syntactic relations form a tree, every word has exactly one head. We can therefore iterate over the arcs in the tree by iterating over the words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependency_graph = np.zeros((len(test_words), len(test_words)))\n",
    "translation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "for i in range(len(lines_test)):\n",
    "    sent1, sent2 = lines_test[i].split('\\t')\n",
    "    sent1 = ' '.join([w for w in sent1.split()])\n",
    "    sent2 = ' '.join([w for w in sent2.split()])\n",
    "    sent1 = nlp(sent1)\n",
    "    sent2 = nlp(sent2)\n",
    "\n",
    "    for s in [sent1, sent2]:\n",
    "        for w in s: #iteration over the words in the sentence\n",
    "            w1 = w.text.lower().translate(translation_table)\n",
    "            w2 = w.head.text.lower().translate(translation_table)\n",
    "\n",
    "            if (not w1 in stopwords_en) & (not w2 in stopwords_en) & (w.dep_ != \"ROOT\") & (w.dep_ != \"punct\"):\n",
    "                w1 = lemmatizer.lemmatize(w1)\n",
    "                w2 = lemmatizer.lemmatize(w2)\n",
    "                if (w1 in test_words) & (w2 in test_words):\n",
    "                    dependency_graph[test_words[w1]][test_words[w2]] += 1\n",
    "                    dependency_graph[test_words[w2]][test_words[w2]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show you the number of words in syntactic dependacies with \"person\" in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(dependency_graph[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Combine synonym, window graphs and dependency graph\n",
    "\n",
    "Now, in order to use the information on synonyms, word order in sentences and syntactic dependencies, we put 3 graps together.\n",
    "\n",
    "Then we complete distance matrix with shortest path lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(687, 687) (687, 687) (687, 687)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 687/687 [00:00<00:00, 1413.06it/s]\n",
      "100%|██████████| 687/687 [00:02<00:00, 343.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances graph is connected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(synonym_graph.shape, window_graph.shape, dependency_graph.shape)\n",
    "distances = 1 / (synonym_graph + window_graph + dependency_graph + 1)\n",
    "#gr_max = np.max(synonym_graph + window_graph + dependency_graph)\n",
    "#distances = gr_max - (synonym_graph + window_graph + dependency_graph) + 1\n",
    "for i in range(distances.shape[0]):\n",
    "    for j in range(distances.shape[0]):\n",
    "        if distances[i][j] == 1:\n",
    "            distances[i][j] = -1.0\n",
    "\n",
    "np.fill_diagonal(distances, 0)\n",
    "\n",
    "def floyd_warshall(dist):\n",
    "    n = dist.shape[0]\n",
    "    for u in tqdm.trange(n):\n",
    "        for v in range(n):\n",
    "            if dist[u][v] == -1.0:\n",
    "                dist[u][v] = 10e6\n",
    "\n",
    "    @numba.jit\n",
    "    def h(dist, n, z):\n",
    "        for u in range(n):\n",
    "            for v in range(n):\n",
    "                if (dist[u][v] > dist[u][z] + dist[z][v]):\n",
    "                    dist[u][v] = dist[u][z] + dist[z][v]\n",
    "\n",
    "    for z in tqdm.trange(n):\n",
    "        h(dist, n, z)\n",
    "        \n",
    "    \n",
    "floyd_warshall(distances)\n",
    "\n",
    "if np.max(distances) == 10e6:\n",
    "    print(\"Distances graph is not connected\")\n",
    "else:\n",
    "    print(\"Distances graph is connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Distance Geometry\n",
    "\n",
    "In this part we produce an approximate realisation of $x$.\n",
    "\n",
    "We have an approximate EDMs $\\tilde{D}$ from raw data. Then we compute an approximate Gram matrix:\n",
    "\n",
    "$$ \\tilde{G} = - \\frac{1}{2} J \\tilde{D}^2 J, $$\n",
    "\n",
    "where $J = I_n - \\frac{1}{n}\\mathbb{1}\\mathbb{1}^{T}$.\n",
    "\n",
    "Having spectral decomposition of $\\tilde{G} = P \\tilde{\\Lambda} P^{T}$, we find $\\Lambda$ the PSD diagonal matrix closest to $\\tilde{\\Lambda}$.\n",
    "\n",
    "$\\Lambda$ is obtained from $\\tilde{\\Lambda}$ by zeroing negative components.\n",
    "\n",
    "So, finally, $x = P \\sqrt{\\Lambda}$.\n",
    "\n",
    "To move $x$ to $0$, we reduce it by its mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def mds(matD, k):\n",
    "    N = matD.shape[0]\n",
    "    matJ = np.eye(N) - np.ones((N, N)) / N\n",
    "    matG = -0.5 * matJ @ (matD ** 2) @ matJ\n",
    "    vecL, matP = la.eigh(matG)\n",
    "    vecL[vecL < 0] = 0\n",
    "    return matP[:,-k:], vecL[-k:]\n",
    "\n",
    "def embed_graph(distances, k):\n",
    "    p, l = mds(distances, k)\n",
    "    result = p @ np.diag(np.sqrt(l))\n",
    "    result -= np.mean(result, axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 687/687 [00:01<00:00, 473.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "with open('dist_graph.dat', 'w') as f:\n",
    "    #sys.stdout = f # Change the standard output to the file we created.\n",
    "    f.write(\"param Kdim := 3;\\nparam n := {};\\nparam : E : c I :=\\n\".format(distances.shape[0]))\n",
    "    for i in tqdm.trange(distances.shape[0]):\n",
    "        for j in range(distances.shape[0]):\n",
    "            if i-j==0:\n",
    "                continue\n",
    "            f.write(\"{} {} {} 1\\n\".format(i+1, j+1, distances[i, j]))\n",
    "    f.write(\";\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "* Please make sure that the AMPL folder is in the system search path, or      *\n",
      "* specify the path via:                                                       *\n",
      "*     AMPL(Environment('full path to the AMPL installation directory'))       *\n",
      "*******************************************************************************\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "AMPL could not be started. Message from process thread:\ncannot execute ampl: No such file or directory\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e6722be12b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mamplpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAMPL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAMPL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dgp1.mod\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dist_graph.dat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/amplpy/ampl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, environment, langext)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamplpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAMPL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AMPL could not be started'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: AMPL could not be started. Message from process thread:\ncannot execute ampl: No such file or directory\n\n"
     ]
    }
   ],
   "source": [
    "from amplpy import AMPL\n",
    "import numpy as np\n",
    "lp = AMPL()\n",
    "lp.read(\"dgp1.mod\")\n",
    "lp.readData(\"dist_graph.dat\")\n",
    "lp.setOption(\"solver\", \"baron\")\n",
    "lp.setOption(\"baron_options\", \"outlev=1 epsr=1e-3 prfreq=1e3 maxtime=30\")\n",
    "lp.solve()\n",
    "ndata = lp.getData(\"n\")\n",
    "n = int(ndata.getRowByIndex(0)[0])\n",
    "solveres = lp.getData(\"solve_result\")\n",
    "solve_result = solveres.getRowByIndex(0)[0]\n",
    "objfun = lp.getObjective(\"objfun\")\n",
    "objfunval = objfun.value()\n",
    "xvar = lp.getVariable(\"x\")\n",
    "x = np.zeros(n)\n",
    "for j in range(n):\n",
    "    x[j] = xvar[j+1].value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualisation in 2D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next plot gives us a visualisation of the result on 2D. Then we will use k = 20 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=20\n",
    "word_to_vec = embed_graph(distances, k)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(*embed_graph(distances, 2).transpose())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMPL methods for subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_printed_sents = 20\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "with open('train.txt') as f:\n",
    "    train = np.array([float(line.strip()) for line in f])\n",
    "ampl_res = []\n",
    "\n",
    "for i, (s1, s2) in enumerate(sentences_test):\n",
    "    \n",
    "    subgraph1 = distances[np.ix_(s1, s1)]\n",
    "    subgraph2 = distances[np.ix_(s2, s2)]\n",
    "    #result1 = ampl\n",
    "    #result2 = ampl\n",
    "    v1 = np.mean(result1, axis=0)\n",
    "    v2 = np.mean(result2, axis=0)\n",
    "    print(v1.shape())\n",
    "    d = (cosine_similarity([v1], [v2])[0,0]+1)/2.\n",
    "    ampl_res.append(d)\n",
    "    #ampl command\n",
    "    #print(d.shape)\n",
    "    #if similarity < 0.4:\n",
    "    print(i, \"{:1.8f} {:1.8f} {:1.8f}\".format(train[i] / 5, d), lines_test[i], sep='\\t')\n",
    "    if i > N_printed_sents:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_xy = np.corrcoef(train, ampl_res)\n",
    "print(\"p_xy spacy train = \", p_xy[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.48000000 0.92187823 0.92373690\tA person is on a baseball team.\tA person is playing basketball on a team.\n",
      "1\t0.04000000 0.35590817 0.63757388\tOur current vehicles will be in museums when everyone has their own aircraft.\tThe car needs to some work\n",
      "2\t0.20000000 0.52993919 0.83476240\tA woman supervisor is instructing the male workers.\tA woman is working as a nurse.\n",
      "3\t0.40000000 0.97916754 0.81605440\tA bike is next to a couple women.\tA child next to a bike.\n",
      "4\t0.44000000 0.28898951 0.82062957\tThe group is eating while taking in a breathtaking view.\tA group of people take a look at an unusual tree.\n",
      "5\t0.68000000 0.99434384 0.95368330\tThe boy is raising his hand.\tThe man is raising his hand.\n",
      "6\t0.16000000 0.69519802 0.81375333\tA man with a gray beard is being shaved in front of a lecture hall\tA man with a beard is sitting in the grass.\n",
      "7\t0.08000000 0.67610976 0.79174104\tThe sky has very little to no clouds.\tThis Lady might be ready for Rock Climbing, or just watching the Clouds, above.\n",
      "8\t0.08000000 0.51193864 0.82175876\tThe young boy jumps barefoot outside in the front yard.\tThe teen rode his bike around the people walking in the courtyard.\n",
      "9\t0.80000000 0.98772491 1.00000000\tThere are dogs in the forest.\tThe dogs are alone in the forest.\n",
      "10\t0.68000000 0.26814506 0.89715342\tSome cyclists stop near a sign.\tTwo men stop to talk near a sign outside.\n",
      "11\t0.60000000 0.98369585 0.89762799\tThese cooks in the white are busy in the kitchen making dinner for their customers.\tThe women are preparing dinner in their kitchen.\n",
      "12\t0.92000000 0.99066898 0.96076984\tA young person  deep in thought.\tA young man deep in thought.\n",
      "13\t0.36000000 1.00000000 0.99999995\tA man is carrying a canoe with a dog.\tA dog is carrying a man in a canoe.\n",
      "14\t0.56000000 0.99475267 0.88919104\tA man is performing labor.\tA man is performing today.\n",
      "15\t0.40000000 0.90485299 0.90401770\tTwo men wearing traditional clothing is standing outside.\tThree women wearing black vests and gray shirts are talking outside of a building.\n",
      "16\t0.44000000 0.86276552 0.81789478\ta woman watches a rap group live\tAn audience watches a girl dance.\n",
      "17\t0.72000000 0.99846045 0.96524848\tA girl dancing on a sandy beach.\tA girl is on a sandy beach.\n",
      "18\t0.52000000 0.89693454 0.90722572\ta man wearing a gray hat fishing out of a fishing boat.\tA man wearing a straw hat and fishing vest in a stream.\n",
      "19\t0.04000000 0.64112040 0.81878744\tA little girl in an orange striped outfit is airborne whilst bouncing on a bed.\tA dog in a red shirt is chasing a squirrel through the glass.\n",
      "20\t0.28000000 0.85226500 0.86201471\tA person is watching people ski down the hill.\tThe woman is jumping a long distance while people watch.\n",
      "21\t0.32000000 0.96331864 0.87245755\tThe yard has a dog.\tThe dog is running after another dog.\n"
     ]
    }
   ],
   "source": [
    "N_printed_sents = 20\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "with open('train.txt') as f:\n",
    "    train = np.array([float(line.strip()) for line in f])\n",
    "\n",
    "for i, (s1, s2) in enumerate(sentences_test):\n",
    "    v1 = np.mean(word_to_vec[s1,:], axis=0)\n",
    "    v2 = np.mean(word_to_vec[s2,:], axis=0)\n",
    "    d = (cosine_similarity([v1], [v2])[0,0]+1)/2.\n",
    "    #print(d.shape)\n",
    "    sent1, sent2 = lines_test[i].split('\\t')\n",
    "    sent1 = nlp(sent1)\n",
    "    sent1 = nlp(' '.join([str(t) for t in sent1 if not t.is_stop]))\n",
    "    sent2 = nlp(sent2)\n",
    "    sent2 = nlp(' '.join([str(t) for t in sent2 if not t.is_stop]))\n",
    "    #sent1 = nlp(' '.join([word for word in sent1 if word not in english_stopwords]))\n",
    "    #sent2 = nlp(' '.join([word for word in sent2 if word not in english_stopwords]))\n",
    "    similarity = sent1.similarity(sent2)\n",
    "    #if similarity < 0.4:\n",
    "    print(i, \"{:1.8f} {:1.8f} {:1.8f}\".format(train[i] / 5, d, similarity), lines_test[i], sep='\\t')\n",
    "    if i > N_printed_sents:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9237369  0.63757388 0.8347624  0.8160544  0.82062957 0.9536833\n",
      " 0.81375333 0.79174104 0.82175876 1.        ]\n",
      "[0.92187823 0.35590817 0.52993919 0.97916754 0.28898951 0.99434384\n",
      " 0.69519802 0.67610976 0.51193864 0.98772491]\n"
     ]
    }
   ],
   "source": [
    "our_simlarity_n = 1-np.array([dist.cosine(np.mean(word_to_vec[s1,:], axis=0), np.mean(word_to_vec[s2,:], axis=0)) for (s1, s2) in sentences_test])/2\n",
    "spacy_similarity_n = np.array([nlp(' '.join([str(t) for t in nlp(sent2) if not t.is_stop])).similarity(nlp(' '.join([str(t) for t in nlp(sent1) if not t.is_stop]))) for (sent1, sent2) in [s.split('\\t') for s in lines_test]])\n",
    "print(spacy_similarity_n[:10])\n",
    "\n",
    "print(our_simlarity_n[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pearson correlation coefficient\n",
    "To estimate the quality of models we will use this coefficient, which is calculated as follows:\n",
    "$$p_{XY} = \\frac{\\sigma_{XY}}{\\sigma_X \\cdot \\sigma_Y}$$\n",
    "where $p_{XY}$ is the Pearson correlation coefficient, $\\sigma_Y, \\sigma_X$ are standart deviations of $Y$ and $X$, and $\\sigma_{XY}$ is the covariance between $X$ and $Y$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_xy spacy train =  0.7131287525074835\n",
      "p_xy spacy our_simlarity_n =  0.5395828515504087\n",
      "p_xy our_simlarity_n train =  0.5113422409775331\n"
     ]
    }
   ],
   "source": [
    "#we normalize all the \n",
    "p_xy = np.corrcoef(train, spacy_similarity_n)\n",
    "print(\"p_xy spacy train = \", p_xy[1, 0])\n",
    "p_xy = np.corrcoef(our_simlarity_n, spacy_similarity_n)\n",
    "print(\"p_xy spacy our_simlarity_n = \", p_xy[1, 0])\n",
    "p_xy = np.corrcoef(train, our_simlarity_n)\n",
    "print(\"p_xy our_simlarity_n train = \", p_xy[1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16965264574028385\n",
      "0.007678251653702461\n",
      "0.7751361274991766\n"
     ]
    }
   ],
   "source": [
    "train_n = train/5  \n",
    "spacy_similarity_scaled = (spacy_similarity_n-0.35)/0.65\n",
    "print(np.mean(((spacy_similarity_scaled-train_n)**2)))\n",
    "\n",
    "print(np.mean(((spacy_similarity_n-np.mean(spacy_similarity_n))**2)))\n",
    "print(np.mean(spacy_similarity_n**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16965264574028385\n",
      "0.007678251653702461\n",
      "0.7751361274991766\n",
      "n\n",
      "0.10256349803064416\n",
      "0.2299975111999682\n",
      "0.06236852391124974\n",
      "0.02809066985255107\n",
      "0.08105832959999999\n"
     ]
    }
   ],
   "source": [
    "train_n = train/5  \n",
    "spacy_similarity_scaled = (spacy_similarity_n-0.35)/0.65\n",
    "print(np.mean(((spacy_similarity_scaled-train_n)**2)))\n",
    "\n",
    "print(np.mean(((spacy_similarity_n-np.mean(spacy_similarity_n))**2)))\n",
    "print(np.mean(spacy_similarity_n**2))\n",
    "\n",
    "print('n')\n",
    "print(np.mean(((our_simlarity_n**4-train_n)**2)))\n",
    "print(np.mean(((spacy_similarity_n-train_n)**2)))\n",
    "print(np.mean(((spacy_similarity_n**4-train_n)**2)))\n",
    "print(np.mean(((spacy_similarity_n-our_simlarity_n)**2)))\n",
    "t = np.mean(train_n)\n",
    "print(np.mean(((train_n-t)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10256349803064416\n",
      "0.2299975111999682\n",
      "0.06236852391124974\n",
      "0.02809066985255107\n",
      "0.08105832959999999\n"
     ]
    }
   ],
   "source": [
    " # _n for normalized\n",
    "print(np.mean(((our_simlarity_n**4-train_n)**2)))\n",
    "print(np.mean(((spacy_similarity_n-train_n)**2)))\n",
    "print(np.mean(((spacy_similarity_n**4-train_n)**2)))\n",
    "print(np.mean(((spacy_similarity_n-our_simlarity_n)**2)))\n",
    "t = np.mean(train_n)\n",
    "print(np.mean(((train_n-t)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f07a3c1acea75c2d4c353e9cd3dd42ec72fb863baf9770b89be62cf6c327390c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
